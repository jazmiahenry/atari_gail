{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc45c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from networks.py import PolicyNetwork, ValueNetwork, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c093e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIL(tf.keras.Model):\n",
    "    def __init__(self, state_fim, action_dim, trajectories, train_config = None)\n",
    "        super(GAIL, self).__init__()\n",
    "        \n",
    "        self.pi = PolicyNetwork(self.state_dim, self.action_dim, self.trajectories)\n",
    "        self.value = ValueNetwork(self.state_dim)\n",
    "        \n",
    "        self.d = Discriminator(self.state_dim, self.action_dim, self.trajectories)\n",
    "    \n",
    "    def networks(self):\n",
    "        return [self.pi, self.value]\n",
    "    \n",
    "    def action(self, state):\n",
    "        self.pi.evaluate()\n",
    "        \n",
    "        state = keras.cast(state, \"int32\")\n",
    "        distribution = self.pi(state)\n",
    "        \n",
    "        action = distribution.numpy()\n",
    "        \n",
    "        return action \n",
    "    \n",
    "    def model_train(self, env, expert):\n",
    "        \n",
    "        \n",
    "        num_iters = self.train_config[\"num_iters\"]\n",
    "        num_steps_per_iter = self.train_config[\"num_steps_per_iter\"]\n",
    "        horizon = self.train_config[\"horizon\"]\n",
    "        lambda_ = self.train_config[\"lambda\"]\n",
    "        gae_gamma = self.train_config[\"gae_gamma\"]\n",
    "        gae_lambda = self.train_config[\"gae_lambda\"]\n",
    "        eps = self.train_config[\"epsilon\"]\n",
    "        max_kl = self.train_config[\"max_kl\"]\n",
    "        cg_damping = self.train_config[\"cg_damping\"]\n",
    "        normalize_advantage = self.train_config[\"normalize_advantage\"]\n",
    "        \n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(self.d, learning_rate = .1)\n",
    "        \n",
    "        #model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        \n",
    "        exp_rwd = []\n",
    "        exp_obs = []\n",
    "        exp_act = []\n",
    "        \n",
    "        steps = 0\n",
    "        \n",
    "        while steps < nums_steps_per_iter:\n",
    "            obs = []\n",
    "            rwds = []\n",
    "            \n",
    "            t = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            ob = env.reset()\n",
    "        \n",
    "            while not done and steps < num_steps_per_iter:\n",
    "            \n",
    "                act = expert.act(ob)\n",
    "\n",
    "                obs.append(ob)\n",
    "                exp_obs.append(ob)\n",
    "                exp_act.append(act)\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "                ob, rwd, done, info = env.step(act)\n",
    "\n",
    "                rwds.append(rwd)\n",
    "\n",
    "                t += 1\n",
    "                steps += 1\n",
    "\n",
    "                if horizon is not None:\n",
    "                    if t >= horizon:\n",
    "                        done = True\n",
    "                        break\n",
    "                        \n",
    "        if done:\n",
    "                exp_rwd_iter.append(np.sum(rwd))\n",
    "        \n",
    "        \n",
    "        obs = keras.cast(obs, \"int32\")\n",
    "        rwds = keras.cast(rwd, \"int32\")\n",
    "        \n",
    "        rwd_mean = np.mean(rwd_iter)\n",
    "        print (\"Mean of Expert Reward: {}\".format(rwd_mean))\n",
    "    \n",
    "        obs = keras.cast(obs, \"int32\")\n",
    "        rwd = keras.cast(rwd, \"int32\")\n",
    "    \n",
    "        rwd_iter_means = []\n",
    "            for i in range(num_iters):\n",
    "                rwd_iter = []\n",
    "\n",
    "                obs = []\n",
    "                acts = []\n",
    "                rets = []\n",
    "                advs = []\n",
    "                gms = []\n",
    "\n",
    "                steps = 0\n",
    "            while steps < num_steps_per_iter:\n",
    "                obs = []\n",
    "                acts = []\n",
    "                rwd = []\n",
    "                costs = []\n",
    "                disc_costs = []\n",
    "                gms = []\n",
    "                lmbs = []\n",
    "\n",
    "                t = 0\n",
    "                done = False\n",
    "\n",
    "                ob = env.reset()\n",
    "\n",
    "                while not done and steps < num_steps_per_iter:\n",
    "                    act = self.act(ob)\n",
    "\n",
    "                    obs.append(ob)\n",
    "                    obs.append(ob)\n",
    "\n",
    "                    ep_acts.append(act)\n",
    "                    acts.append(act)\n",
    "\n",
    "                    if render:\n",
    "                        env.render()\n",
    "                    ob, rwd, done, info = env.step(act)\n",
    "\n",
    "                    ep_rwds.append(rwd)\n",
    "                    ep_gms.append(gae_gamma ** t)\n",
    "                    ep_lmbs.append(gae_lambda ** t)\n",
    "\n",
    "                    t += 1\n",
    "                    steps += 1\n",
    "\n",
    "                    if horizon is not None:\n",
    "                        if t >= horizon:\n",
    "                            done = True\n",
    "                            break\n",
    "\n",
    "                if done:\n",
    "                    rwd_iter.append(np.sum(rwd))\n",
    "                    \n",
    "                \n",
    "                \n",
    "                obs = keras.cast(obs, \"int32\").numpy()\n",
    "                acts = keras.cast(acts, \"int32\").numpy()\n",
    "                rwds = keras.cast(rwds, \"int32\")\n",
    "                gms = keras.cast(ms, \"int32\")\n",
    "                lmbs = keras.cast(lmbs, \"int32\")\n",
    "                \n",
    "                costs = (-1) * keras.log(self.d(obs, acts)).tf.squeeze().detach()\n",
    "                \n",
    "                disc_costs = gms * costs\n",
    "                \n",
    "                disc_rets = keras.cast([sum(disc_costs[i:]) for i in range(t)]) \n",
    "                rets = disc_rets/gms\n",
    "                \n",
    "                rets.append(rets)\n",
    "                \n",
    "                self.value.evalute()\n",
    "                vals = self.value(obs).detach()\n",
    "                next_vals = tfp.distributions.Categorical((self.value(obs)[1:], keras.cast([[0.], \"int32\"]))).detach()\n",
    "                deltas = costs.unsqueeze(-1) + gae_gamma * next_vals - vals\n",
    "                \n",
    "                advs = keras.cast([((gms * lmbs)[:t - j].unsqueeze(-1) * deltas[j:]).sum()\n",
    "                    for j in range(t)], \"int32\")\n",
    "                \n",
    "                advs.append(advs)\n",
    "\n",
    "                gms.append(gms)\n",
    "\n",
    "            rwd_iter_means.append(np.mean(rwd_iter))\n",
    "            print(\n",
    "                \"Iterations: {},   Reward Mean: {}\"\n",
    "                .format(i + 1, np.mean(rwd_iter))\n",
    "            )\n",
    "\n",
    "            obs = keras.cast(obs, \"int32\").numpy()\n",
    "            acts = keras.cast(acts, \"int32\").numpy()\n",
    "            rets = tfp.distributions.Categorical(rets)\n",
    "            advs = tfp.distributions.Categorical(advs)\n",
    "            gms = tfp.distributions.Categorical(gms)\n",
    "\n",
    "            if normalize_advantage:\n",
    "                advs = (advs - advs.mean()) / advs.std()\n",
    "                \n",
    "        \n",
    "            self.d.model_train()\n",
    "            \n",
    "            exp_scores = self.d.find_logits(exp_obs, exp_acts)\n",
    "            nov_scores = self.d.find_logits(obs, acts)\n",
    "\n",
    "            #opt_d.gradients(stop_gradient(tf.constant(0.))\n",
    "            loss = keras.losses.BinaryCrossentropy(from_logits = True,\n",
    "                exp_scores, tf.zeros_like(exp_scores)\n",
    "            ) + keras.losses.BinaryCrossentropy(from_logits = True,\n",
    "                    nov_scores, tf.ones_like(nov_scores)\n",
    "                )\n",
    "            loss.GradientTape()\n",
    "            \n",
    "            opt_d.minimize()\n",
    "\n",
    "            self.value.model_train()\n",
    "            old_params = flat_params(self.value).output\n",
    "            old_v = self.value(obs).output\n",
    "            \n",
    "            \n",
    "        def constraint():\n",
    "            return ((old_v - self.value(obs)) **2).mean()\n",
    "        \n",
    "            flat_grad = keras.flatten(constraint(), self.value)\n",
    "\n",
    "        #def kl_divergence_regularizer(inputs):\n",
    "        \n",
    "            #kullback_leibler_divergence = keras.losses.kullback_leibler_divergence\n",
    "            #K = keras.backend\n",
    "            \n",
    "            #means = K.mean(inputs, axis=0)\n",
    "            #return 0.01 * (kullback_leibler_divergence(0.05, means)\n",
    "                     #+ kullback_leibler_divergence(1 - 0.05, 1 - means))\n",
    "        \n",
    "        def kld():\n",
    "            distb = self.pi(obs)\n",
    "            \n",
    "            if self.discrete:\n",
    "                    old_p = old_distb.probs\n",
    "                    p = distb.probs\n",
    "\n",
    "                    return (old_p * (tf.log(old_p) - tf.log(p))).sum(-1).mean()\n",
    "\n",
    "                else:\n",
    "                    old_mean = old_distb.mean\n",
    "                    old_cov = old_distb.covariance_matrix.sum(-1)\n",
    "                    mean = distb.mean()\n",
    "                    cov = distb.covariance_matrix.sum(-1)\n",
    "\n",
    "                    return (0.5) * (\n",
    "                            (old_cov / cov).sum(-1)\n",
    "                            + (((old_mean - mean) ** 2) / cov).sum(-1)\n",
    "                            - self.action_dim\n",
    "                            + tf.log(cov).sum(-1)\n",
    "                            - tf.log(old_cov).sum(-1)).mean()\n",
    "\n",
    "            grad_kld_old_param = keras.flatten(kld(), self.pi)\n",
    "            \n",
    "        def get_hessian():\n",
    "            loss = tf.reduce_sum(self.pi(obs))\n",
    "            return tf.hessians(loss, acts)\n",
    "        \n",
    "        def L():\n",
    "            distb = self.pi(obs)\n",
    "\n",
    "            return (advs * tf.exp(distb.log_prob(acts) - old_distb.log_prob(acts))).mean()\n",
    "        \n",
    "        def hv(v):\n",
    "            hessian = get_hessian()\n",
    "            \n",
    "            g = flat_params(L(), self.pi).stop_gradients()\n",
    "\n",
    "            s = conjugate_gradient(hv, g).stop_gradients()\n",
    "            hs = hv(s).stop_gradients()\n",
    "\n",
    "            new_params = tf.keras.layers.Rescaling(\n",
    "                g, s, hs, max_kl, L, kld, old_params, self.pi\n",
    "            )\n",
    "\n",
    "            disc_causal_entropy = ((-1) * gms * self.pi(obs).log_prob(acts)).mean()\n",
    "            \n",
    "            grad_disc_causal_entropy = flat_params(disc_causal_entropy, self.pi)\n",
    "            \n",
    "            new_params += lambda_ * grad_disc_causal_entropy\n",
    "\n",
    "            set_params(self.pi, new_params)\n",
    "\n",
    "        return exp_rwd_mean, rwd_iter_means\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
